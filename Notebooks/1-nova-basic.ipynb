{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOVA IS IMPLEMENTED WITH DOCUMENT INTELLIGENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PDFPlumberLoader # Load the document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # split the documkent\n",
    "from langchain_ollama import OllamaEmbeddings  # Embed the documents\n",
    "from langchain_core.vectorstores import InMemoryVectorStore # store the data\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain # For combining all the documents\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sumi/Desktop/NOVA/Notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sumi/Desktop/NOVA\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the API Keys\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "tavily_api_key=os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document\n",
    "loader=PDFPlumberLoader('/Users/sumi/Desktop/NOVA/HSI-project-document.pdf')\n",
    "raw_docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/sumi/Desktop/NOVA/HSI-project-document.pdf', 'file_path': '/Users/sumi/Desktop/NOVA/HSI-project-document.pdf', 'page': 0, 'total_pages': 3, 'Title': 'HSI-project-document', 'Producer': 'Skia/PDF m134 Google Docs Renderer', 'start_index': 0}, page_content='HyperSpectral Image Classification\\nProject overview: A HyperSpectral Image(HSI) is a high-dimensionality\\ndataset that captures images across numerous spectral bands, beyond the\\nvisible spectrum. Unlike RGB images (3 bands), HSI consists of hundreds\\nof contiguous bands spanning in ultraviolet, visible, infrared and beyond\\nregions.\\nThis High-dimensional nature of HSI data makes it challenging for\\ntraditional machine learning models to capture intricate spectral-spatial\\nrelationships. Hence we propose deep learning based approaches to\\naccurately classify HSI data.'),\n",
       " Document(metadata={'source': '/Users/sumi/Desktop/NOVA/HSI-project-document.pdf', 'file_path': '/Users/sumi/Desktop/NOVA/HSI-project-document.pdf', 'page': 1, 'total_pages': 3, 'Title': 'HSI-project-document', 'Producer': 'Skia/PDF m134 Google Docs Renderer', 'start_index': 0}, page_content='These workflows when run independently work fine and produce expected\\nresults.\\nThe Problem: The key challenge in integrating this project into an MLOps\\npipeline arises from the AutoEncoder-based approach. The\\nAutoEncoder is utilized for dimensionality reduction, making it part of the\\ndata_transformation step. However, for dimensionality reduction to be\\neffective, the AutoEncoder must first be trained, which is typically done in\\nthe model_trainer step.\\nThis creates a circular dependency:\\n● The model_trainer step needs to be executed before\\ndata_transformation, so the AutoEncoder can learn meaningful\\nrepresentations.'),\n",
       " Document(metadata={'source': '/Users/sumi/Desktop/NOVA/HSI-project-document.pdf', 'file_path': '/Users/sumi/Desktop/NOVA/HSI-project-document.pdf', 'page': 2, 'total_pages': 3, 'Title': 'HSI-project-document', 'Producer': 'Skia/PDF m134 Google Docs Renderer', 'start_index': 0}, page_content='● After transformation, the model_trainer step needs to be executed\\nagain to fine-tune the classification model using the transformed data.\\nData Ingestion --> Model Training (AutoEncoder) --> Data Transformation\\n--> Model Training (AutoEncoder+Classifier) --> Prediction\\nProject structure:\\nHyperSpec-AI\\n│-- config/ # Configuration files (e.g., paths, parameters)\\n│ └── config.yaml\\n│-- DATA/ # Dataset storage\\n│-- Notebooks/ # Jupyter Notebooks for research & experimentation\\n│-- src/\\n│ ├── components/ # Core components of the pipeline\\n│ │ ├── data_ingestion.py # Handles dataset loading\\n│ │ ├── data_transformation.py # Applies transformations (e.g.,\\nAutoEncoder)\\n│ │ ├── model_trainer.py # Trains models (CNN, AutoEncoder,\\nClassifier)\\n│ ├── models/ # Model architectures\\n│ │ ├── cnn_model.py # Convolutional Neural Network\\n│ │ ├── autoencoder_classifier_head.py # AutoEncoder + Classifier\\n│ ├── pipeline/ # End-to-end pipeline execution\\n│ │ ├── training_pipeline.py # Full training pipeline'),\n",
       " Document(metadata={'source': '/Users/sumi/Desktop/NOVA/HSI-project-document.pdf', 'file_path': '/Users/sumi/Desktop/NOVA/HSI-project-document.pdf', 'page': 2, 'total_pages': 3, 'Title': 'HSI-project-document', 'Producer': 'Skia/PDF m134 Google Docs Renderer', 'start_index': 825}, page_content='│ │ ├── autoencoder_classifier_head.py # AutoEncoder + Classifier\\n│ ├── pipeline/ # End-to-end pipeline execution\\n│ │ ├── training_pipeline.py # Full training pipeline\\n│ │ ├── predict_pipeline.py # Inference pipeline\\n│ └── utils.py # Utility functions (logging, preprocessing, etc.)\\nThe logs and artifacts will be automatically generated.\\nPlease read the logs for better understandability of the project flow.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the document\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                              chunk_overlap=200,\n",
    "                                              add_start_index=True)\n",
    "\n",
    "document_chunks=text_splitter.split_documents(raw_docs)\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_core.vectorstores.in_memory.InMemoryVectorStore at 0x10d37bdc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed the documents and store in vector store\n",
    "EMBEDDING_MODEL = OllamaEmbeddings(model=\"deepseek-r1:1.5b\")\n",
    "vector_store=InMemoryVectorStore.from_documents(document_chunks,EMBEDDING_MODEL) # Any DB like FAISS, CHROMA, ASTRA can be used\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'● After transformation, the model_trainer step needs to be executed\\nagain to fine-tune the classification model using the transformed data.\\nData Ingestion --> Model Training (AutoEncoder) --> Data Transformation\\n--> Model Training (AutoEncoder+Classifier) --> Prediction\\nProject structure:\\nHyperSpec-AI\\n│-- config/ # Configuration files (e.g., paths, parameters)\\n│ └── config.yaml\\n│-- DATA/ # Dataset storage\\n│-- Notebooks/ # Jupyter Notebooks for research & experimentation\\n│-- src/\\n│ ├── components/ # Core components of the pipeline\\n│ │ ├── data_ingestion.py # Handles dataset loading\\n│ │ ├── data_transformation.py # Applies transformations (e.g.,\\nAutoEncoder)\\n│ │ ├── model_trainer.py # Trains models (CNN, AutoEncoder,\\nClassifier)\\n│ ├── models/ # Model architectures\\n│ │ ├── cnn_model.py # Convolutional Neural Network\\n│ │ ├── autoencoder_classifier_head.py # AutoEncoder + Classifier\\n│ ├── pipeline/ # End-to-end pipeline execution\\n│ │ ├── training_pipeline.py # Full training pipeline'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the vector DB\n",
    "query=\"what is the major problem\"\n",
    "result=vector_store.similarity_search(query,k=4)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'● After transformation, the model_trainer step needs to be executed\\nagain to fine-tune the classification model using the transformed data.\\nData Ingestion --> Model Training (AutoEncoder) --> Data Transformation\\n--> Model Training (AutoEncoder+Classifier) --> Prediction\\nProject structure:\\nHyperSpec-AI\\n│-- config/ # Configuration files (e.g., paths, parameters)\\n│ └── config.yaml\\n│-- DATA/ # Dataset storage\\n│-- Notebooks/ # Jupyter Notebooks for research & experimentation\\n│-- src/\\n│ ├── components/ # Core components of the pipeline\\n│ │ ├── data_ingestion.py # Handles dataset loading\\n│ │ ├── data_transformation.py # Applies transformations (e.g.,\\nAutoEncoder)\\n│ │ ├── model_trainer.py # Trains models (CNN, AutoEncoder,\\nClassifier)\\n│ ├── models/ # Model architectures\\n│ │ ├── cnn_model.py # Convolutional Neural Network\\n│ │ ├── autoencoder_classifier_head.py # AutoEncoder + Classifier\\n│ ├── pipeline/ # End-to-end pipeline execution\\n│ │ ├── training_pipeline.py # Full training pipeline'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert it into retriever and query\n",
    "retriever=vector_store.as_retriever()\n",
    "result=retriever.invoke(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nGenerative AI refers to a type of artificial intelligence that can create or generate new content, such as text, images, music, or even code. Unlike traditional AI, which is typically designed to perform specific tasks or make predictions, generative AI models are capable of producing outputs that are similar to those created by humans.\\n\\n### Key Characteristics of Generative AI:\\n1. **Content Creation**: Generative AI can produce new content, such as writing articles, composing music, or generating images.\\n2. **Learning from Data**: These models are trained on large datasets to understand patterns, styles, and structures.\\n3. **Creativity**: They can combine existing information in novel ways to create something unique.\\n4. **Applications**: Generative AI is used in various fields, including art, entertainment, education, and business.\\n\\n### Examples of Generative AI:\\n- **Text Generation**: Chatbots, automated essay writers, and language translation tools.\\n- **Image Generation**: Tools like DALL-E, MidJourney, and Stable Diffusion, which create images from text prompts.\\n- **Music and Sound**: AI-generated music, sound effects, and even entire compositions.\\n- **Code Generation**: Tools that help developers write code by suggesting snippets or completing functions.\\n\\n### How Generative AI Works:\\nGenerative AI often uses machine learning models, particularly **generative adversarial networks (GANs)** and **transformers**. These models are trained on vast amounts of data to learn patterns and then generate new outputs by sampling from the learned distribution.\\n\\n### Applications of Generative AI:\\n1. **Art and Design**: Creating digital art, designing products, and generating 3D models.\\n2. **Entertainment**: Writing scripts, composing music, and producing videos.\\n3. **Education**: Generating educational content, quizzes, and personalized learning materials.\\n4. **Marketing**: Creating ads, social media posts, and product descriptions.\\n5. **Healthcare**: Generating synthetic medical data for research and training purposes.\\n\\n### Ethical Considerations:\\n- **Bias**: Generative AI can inherit biases from its training data, leading to unfair or misleading outputs.\\n- **Misuse**: The technology can be used maliciously, such as creating deepfakes or spreading misinformation.\\n- **Copyright and Ownership**: Questions arise about who owns the rights to AI-generated content.\\n\\nGenerative AI is a rapidly evolving field with immense potential to transform industries, but it also raises important ethical and societal questions that need to be addressed as the technology advances.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 508, 'prompt_tokens': 7, 'total_tokens': 515, 'completion_time': 3.628571429, 'prompt_time': 0.002811103, 'queue_time': 0.057204238, 'total_time': 3.631382532}, 'model_name': 'Deepseek-R1-Distill-Qwen-32b', 'system_fingerprint': 'fp_0852292947', 'finish_reason': 'stop', 'logprobs': None}, id='run-d3ca16dc-1c14-46cd-8046-d008dc6f84ea-0', usage_metadata={'input_tokens': 7, 'output_tokens': 508, 'total_tokens': 515})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the llm\n",
    "llm_engine = ChatGroq(model=\"Deepseek-R1-Distill-Qwen-32b\", groq_api_key=groq_api_key)\n",
    "# provide sample input and get sample response from the LLM\n",
    "result=llm_engine.invoke(\"what is generative AI\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I\\'m trying to understand what LangSmith is. I know it\\'s related to machine learning and AI, but beyond that, I\\'m a bit fuzzy. Let me start by breaking down the name: \"Lang\" probably stands for language, and \"Smith\" is a common last name, but in tech, sometimes it\\'s used as a tool or framework name. Maybe it\\'s a tool for working with languages or something related.\\n\\nI remember that there\\'s a company called Hugging Face, which is known for their work in natural language processing (NLP). They have tools like Hugging Face Transformers, which are used for training and deploying NLP models. Maybe LangSmith is related to them? Or perhaps it\\'s an open-source project.\\n\\nI think I\\'ve heard of LangSmith in the context of creating and managing machine learning pipelines. Pipelines are sequences of data processing steps, right? So LangSmith might help in setting up these pipelines for different tasks, maybe not just NLP but other types of models too.\\n\\nWait, the user mentioned something about an open-source framework. So LangSmith could be a tool that allows developers to build, train, and deploy machine learning models efficiently. It might offer features like model management, data preprocessing, hyperparameter tuning, and integration with various ML frameworks.\\n\\nI should consider what problems LangSmith solves. Maybe it\\'s about simplifying the process of setting up machine learning experiments, making it easier to iterate and test different models. Or perhaps it provides a user-friendly interface for non-technical users to work with ML models.\\n\\nI\\'m also thinking about how LangSmith might fit into the broader AI ecosystem. Does it integrate with popular libraries like TensorFlow, PyTorch, or scikit-learn? That would make it more versatile for developers who use these tools.\\n\\nAnother angle is whether LangSmith has specific features for deployment. For example, maybe it helps in deploying models to production environments, handling scaling, monitoring, and updates. That would be valuable for teams looking to move beyond experimentation to actual deployment.\\n\\nI should also consider any tutorials or documentation available. If LangSmith is widely used, there should be resources to help users get started. Maybe there are examples of how to use LangSmith for specific tasks, like image classification or text generation.\\n\\nI\\'m a bit confused about whether LangSmith is a standalone tool or part of a larger platform. If it\\'s standalone, what makes it different from other tools like MLflow or Kubeflow? If it\\'s part of a platform, how does it complement other tools?\\n\\nI also wonder about the community around LangSmith. Is it actively maintained? Are there regular updates and improvements? A strong community can make a tool more reliable and easier to use.\\n\\nIn summary, LangSmith seems to be a tool related to machine learning, possibly for building and managing ML pipelines, integrating with other libraries, and simplifying the deployment process. It might be an open-source project aimed at making ML workflows more efficient. To get a clearer picture, I should look up the official documentation or any tutorials that explain its features and use cases.\\n\\nWait, I think I might have confused LangSmith with another tool. Let me check my thoughts again. I recall that Hugging Face has a tool called LangSmith, which is designed for creating and managing machine learning applications. It might offer features like model versioning, experiment tracking, and deployment capabilities. This would help data scientists and engineers streamline their ML workflows from development to production.\\n\\nSo, LangSmith could be a comprehensive platform that handles various stages of the ML lifecycle, making it easier for teams to collaborate and manage their projects effectively. It might also provide a user interface for monitoring models and handling data, which would be beneficial for both technical and non-technical users.\\n\\nI should also consider any recent updates or features. For example, does LangSmith support the latest machine learning techniques or frameworks? Is it compatible with cloud services for scalable deployment?\\n\\nIn conclusion, LangSmith is likely a tool designed to simplify and streamline the process of building, training, and deploying machine learning models, possibly with a focus on integration and ease of use. It might be part of a larger ecosystem, offering a range of features to support the entire ML workflow.\\n</think>\\n\\nLangSmith is a tool designed to simplify and streamline the process of building, training, and deploying machine learning models. It is likely part of a larger ecosystem, possibly associated with Hugging Face, and is focused on creating and managing machine learning applications. LangSmith may offer features such as model versioning, experiment tracking, deployment capabilities, and integration with various machine learning frameworks like TensorFlow, PyTorch, and scikit-learn. It aims to support the entire ML workflow, from development to production, and may provide a user interface for monitoring models and handling data, making it beneficial for both technical and non-technical users.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample chain using chatprompt templates and LCEL\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an Expert AI Engineer. Provide me the answer based on the questions\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain= prompt | llm_engine | output_parser\n",
    "\n",
    "response= chain.invoke({\"input\": \"can you tell me about LangSmith\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, I need to answer the user's questions based on the provided context. Let me read through the context again to make sure I understand everything.\\n\\nThe project is about hyperpectral image classification using deep learning. They have a pipeline that includes data ingestion, model training with an AutoEncoder, data transformation using that AutoEncoder, further model training with both the AutoEncoder and a classifier, and finally prediction.\\n\\nThe structure of the project includes several files and folders, like config, DATA, Notebooks, src, and utils. The src folder has components for data ingestion, transformation, and model training, as well as models and pipeline scripts.\\n\\nThe problem they're facing is a circular dependency. The model_trainer step needs to run before data_transformation because the AutoEncoder has to be trained to transform the data. But in their current setup, the model_trainer is part of the pipeline, and the data_transformation is a separate step that uses the trained model. So they can't run model_trainer without the data being transformed, but the data can't be transformed without the model being trained first.\\n\\nI think the user's questions will revolve around understanding this circular dependency, how it affects the pipeline, and possible solutions. They might ask about the project structure, the workflow, the problem they're facing, and how to resolve it.\\n\\nLet me outline potential questions and answers based on the context.\\n\\n1. What is the main challenge in integrating the project into an MLOps pipeline?\\n   - The main challenge is the circular dependency between model training and data transformation. The AutoEncoder needs to be trained before data transformation, but the data transformation is part of the pipeline that comes after model training.\\n\\n2. How does the AutoEncoder contribute to dimensionality reduction?\\n   - The AutoEncoder is trained to learn a compressed representation of the high-dimensional HSI data. Once trained, it's used in the data transformation step to reduce the dimensionality of the data, making it easier for the classifier to process.\\n\\n3. What steps are involved in the project's workflow?\\n   - The workflow includes Data Ingestion, Model Training (AutoEncoder), Data Transformation using the AutoEncoder, further Model Training (AutoEncoder + Classifier), and Prediction.\\n\\n4. How is the project structured?\\n   - The project has several key directories: config, DATA, Notebooks, src, and utils. The src directory includes components for data handling, model training, and the pipeline, along with model architectures and utility functions.\\n\\n5. Why is the circular dependency a problem?\\n   - Because the model_trainer needs to run before data_transformation, but in the pipeline, data_transformation is a step that comes after model_trainer. This creates a loop where each step depends on the other.\\n\\n6. What are the possible solutions to break the circular dependency?\\n   - Possible solutions might include training the AutoEncoder separately before the main pipeline runs, or restructuring the pipeline to handle the training and transformation in a way that doesn't require both steps to be dependent on each other.\\n\\nI should make sure my answers are clear and directly address each potential question. I'll need to explain the problem in a way that's easy to understand, perhaps by breaking it down into steps or components.\\n</think>\\n\\n**Q1: What is the main challenge in integrating the project into an MLOps pipeline?**\\n\\n**A1:** The main challenge is a circular dependency between the model training and data transformation steps. The AutoEncoder must be trained before it can be used for dimensionality reduction in the data transformation step. However, the current pipeline structure places data transformation after model training, creating a loop where each step depends on the other.\\n\\n---\\n\\n**Q2: How does the AutoEncoder contribute to dimensionality reduction in this project?**\\n\\n**A2:** The AutoEncoder is trained to learn a compressed representation of the high-dimensional hyperpectral image (HSI) data. Once trained, it is used in the data transformation step to reduce the dimensionality of the data, making it more manageable for the subsequent classification model.\\n\\n---\\n\\n**Q3: What steps are involved in the project's workflow?**\\n\\n**A3:** The workflow consists of the following steps:\\n\\n1. **Data Ingestion:** Loading the hyperpectral image dataset.\\n2. **Model Training (AutoEncoder):** Training an AutoEncoder for dimensionality reduction.\\n3. **Data Transformation:** Applying the trained AutoEncoder to transform the data.\\n4. **Model Training (AutoEncoder + Classifier):** Fine-tuning the model with both the AutoEncoder and classifier using the transformed data.\\n5. **Prediction:** Using the trained model for classifying new hyperpectral images.\\n\\n---\\n\\n**Q4: How is the project structured?**\\n\\n**A4:** The project is structured with the following key directories and files:\\n\\n- **config/**: Contains configuration files, such as `config.yaml`, which specifies paths and parameters.\\n- **DATA/**: Stores the dataset used for training and inference.\\n- **Notebooks/**: Holds Jupyter Notebooks for research and experimentation.\\n- **src/**: The main source code directory, which includes:\\n  - **components/**: Handles core pipeline components like data ingestion, transformation, and model training.\\n  - **models/**: Contains model architectures, such as the CNN and AutoEncoder + Classifier.\\n  - **pipeline/**: Includes scripts for the full training pipeline (`training_pipeline.py`) and inference (`predict_pipeline.py`).\\n  - **utils.py**: Provides utility functions for logging, preprocessing, etc.\\n\\n---\\n\\n**Q5: Why is the circular dependency a problem in the pipeline?**\\n\\n**A5:** The circular dependency arises because the model_trainer step depends on the data being transformed, which itself requires the AutoEncoder to be trained. However, in the pipeline, the data_transformation step comes after model_trainer, creating a loop where each step requires the other to have already been completed. This makes it difficult to execute the pipeline in a linear, sequential manner.\\n\\n---\\n\\n**Q6: What are the possible solutions to break the circular dependency?**\\n\\n**A6:** Possible solutions to resolve the circular dependency include:\\n\\n1. **Separate Training Phase for AutoEncoder:**\\n   - Train the AutoEncoder independently before the main pipeline runs. This way, the transformed data can be used in subsequent steps without requiring the model_trainer to run first.\\n\\n2. **Refactor the Pipeline:**\\n   - Restructure the pipeline to handle the training of the AutoEncoder and the transformation step in a way that avoids the dependency loop. This might involve separating the AutoEncoder training from the main model training.\\n\\n3. **Use Pre-trained Models:**\\n   - If the AutoEncoder can be pre-trained on a similar dataset, it can be loaded and used for data transformation without needing to train it within the pipeline.\\n\\n4. **Modularize Components:**\\n   - Break down the pipeline into more modular components, allowing the AutoEncoder training and data transformation to be executed as separate tasks that can be orchestrated independently.\\n\\nBy addressing this circular dependency, the pipeline can be made more efficient and suitable for integration into an MLOps framework.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will combine our document chain and retriever chain so that llm can have context about documents\n",
    "prompt= ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following questions based on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm_engine,prompt)\n",
    "\n",
    "# create the retriever chain\n",
    "retrieval_chain=create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Get the response\n",
    "response=retrieval_chain.invoke({\"input\":\"what is the major problem faced in the project\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
